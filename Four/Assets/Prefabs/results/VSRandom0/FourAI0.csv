Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
250000,1.9054375,6.76090669150753,-0.3659865,-0.4440993788819876,-0.4440993788819876,0.50585514,0.01957526,0.00016462388,0.15487462,0.0027482426,1.0
300000,1.903674,6.375221238938053,-0.42513368,-0.5125368731563422,-0.5125368731563422,0.48330307,0.022696441,0.00013696363,0.14565451,0.0022881606,1.0
350000,1.9023699,6.132220795892169,-0.45287758,-0.5407217230067037,-0.5407217230067037,0.46831045,0.023239467,0.00010622935,0.13540974,0.0017769467,1.0
400000,1.9022956,6.008130081300813,-0.46994054,-0.5500420521446594,-0.5500420521446594,0.46053177,0.024584673,7.549532e-05,0.12516508,0.0012657375,1.0
450000,1.902662,5.833424432905165,-0.5073848,-0.5979775895053293,-0.5979775895053293,0.43735376,0.022659281,4.4762604e-05,0.11492084,0.0007545498,1.0
500000,1.9008864,5.834313055365687,-0.51657975,-0.5946684894053315,-0.5946684894053315,0.44425893,0.02672553,1.40313305e-05,0.10467708,0.00024338621,1.0
