Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
200000,1.2464229,2.2246472372411974,0.55120045,0.5912321134202951,0.5912321134202951,0.26902336,0.025313677,0.0001939808,0.16466025,0.0032365462,1.0
250000,1.111064,2.1999232049148856,0.5881271,0.5864470273912626,0.5864470273912626,0.25830805,0.024380373,0.00016632894,0.15544297,0.002776604,1.0
300000,0.9994118,2.1986310133060387,0.5467788,0.5387813459584697,0.5387813459584697,0.28469667,0.025594482,0.00013560224,0.14520071,0.002265516,1.0
350000,0.8915849,2.242267038454056,0.48227566,0.484079826214823,0.484079826214823,0.31332228,0.024307624,0.00010487598,0.13495864,0.0017544361,1.0
400000,0.7092545,2.383407768304236,0.48882723,0.49500160712959507,0.49500160712959507,0.2767207,0.025119698,7.414989e-05,0.124716595,0.0012433582,1.0
450000,0.5992423,2.757345757871797,0.35005334,0.3243680957786248,0.3243680957786248,0.27950615,0.023492396,4.3423403e-05,0.11447444,0.00073227443,1.0
500000,0.5672732,3.222175308224962,0.20343842,0.16811264998593622,0.16811264998593622,0.20220074,0.02329468,1.2696575e-05,0.10423217,0.00022118473,1.0
550000,0.54985845,3.4349831470640413,0.146618,0.047395445403793465,0.047395445403793465,None,None,None,None,None,0.0
600000,0.54483086,3.5132232150916147,0.11883689,-0.03708807188405227,-0.03708807188405227,None,None,None,None,None,0.0
