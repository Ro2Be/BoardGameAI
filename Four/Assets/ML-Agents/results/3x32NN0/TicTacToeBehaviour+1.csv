Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
150000,1.6101822,1.3794139744552967,-0.46294817,-0.5852463892052231,-0.5852463892052231,0.3329866,0.017726593,0.0002178832,0.17262769,0.0036341222,1.0
200000,1.5862758,1.3500188005264147,-0.53304607,-0.5850150991657731,-0.5850150991657731,0.2991882,0.025758024,0.00019637527,0.16545841,0.0032763737,1.0
250000,1.4681354,1.3953243269138642,-0.46783635,-0.5192554134183447,-0.5192554134183447,0.3188153,0.029811893,0.00016564949,0.15521649,0.0027653025,1.0
300000,1.2700759,1.5075225677031092,-0.43167228,-0.4910710255265116,-0.4910710255265116,0.3069866,0.026689168,0.0001349217,0.14497387,0.0022541967,1.0
350000,1.0249523,1.9505488020771864,-0.30539522,-0.32143905616054197,-0.32143905616054197,0.2970384,0.03201395,0.00010419559,0.13473184,0.0017431189,1.0
400000,0.6621603,2.4600373676562177,-0.0706354,-0.09860952458767881,-0.09860952458767881,0.195714,0.02539439,7.346888e-05,0.12448959,0.0012320312,1.0
450000,0.41477576,2.6595915977457367,0.064566076,0.04721785205203915,0.04721785205203915,0.13162619,0.027156483,4.2742286e-05,0.114247404,0.0007209453,1.0
500000,0.29587844,2.7257824143070044,0.12996775,0.1164889167733324,0.1164889167733324,0.11013079,0.022366883,1.5087394e-05,0.1050291,0.000260952,1.0
