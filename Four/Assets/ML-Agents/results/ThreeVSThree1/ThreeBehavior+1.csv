Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.2803535,3.02722293814433,-0.09379883,-0.2940797422472815,-0.2940797422472815,0.7487912,0.024557283,0.00028463447,0.19487815,0.00474442,1.0
100000,1.1606582,2.914044152184124,-0.27453607,-0.38562705495537813,-0.38562705495537813,0.6818479,0.0258053,0.0002569793,0.18565975,0.004284422,1.0
150000,0.88658905,3.362882819998255,-0.17028783,-0.1850623854811971,-0.1850623854811971,0.6789829,0.024037905,0.00022624961,0.17541654,0.0037732844,1.0
200000,0.62758154,3.2964681618973963,-0.14394806,-0.2240268110337716,-0.2240268110337716,0.64921165,0.024883766,0.00019552003,0.16517332,0.0032621485,1.0
250000,0.5758732,3.016305220883534,-0.29569006,-0.3039357429718875,-0.3039357429718875,0.59687555,0.029114464,0.00016479177,0.15493056,0.002751035,1.0
300000,0.55813783,2.702584610827224,0.0632174,0.0352514256091239,0.0352514256091239,0.6306996,0.02922747,0.00013406502,0.14468834,0.0022399472,1.0
350000,0.52076674,2.250975292587776,0.05067463,-0.16495448634590376,-0.16495448634590376,0.63558894,0.027800724,0.00010333878,0.13444623,0.0017288672,1.0
400000,0.57975423,2.842169970800676,0.12695788,0.07199938527739358,0.07199938527739358,0.6086881,0.027137827,7.261028e-05,0.124203406,0.0012177496,1.0
450000,0.43396798,3.080633314290378,0.33542243,0.33404064310781034,0.33404064310781034,0.49199665,0.024519492,4.495298e-05,0.114984296,0.0007577166,1.0
500000,0.3088295,3.0862209872507353,0.5659754,0.5684047074207257,0.5684047074207257,0.3902487,0.022938306,1.7296656e-05,0.10576554,0.00029769941,1.0
