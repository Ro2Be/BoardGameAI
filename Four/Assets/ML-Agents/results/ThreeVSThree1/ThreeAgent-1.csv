Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.2595512,3.700507614213198,0.1835599,0.29096549779073044,0.29096549779073044,0.691589,0.02681603,0.00028463564,0.19487855,0.0047444394,1.0
100000,1.0865543,3.5628764373060777,0.36019343,0.39085599561963863,0.39085599561963863,0.60824287,0.026896376,0.00025697966,0.1856599,0.0042844275,1.0
150000,0.87498784,3.8387689925481467,0.3032073,0.251524242717507,0.251524242717507,0.6132002,0.025118863,0.00022624937,0.17541644,0.0037732807,1.0
200000,0.752551,3.970968383376417,0.18530563,0.14814078345595547,0.14814078345595547,0.62212676,0.025274348,0.00019551942,0.16517313,0.0032621392,1.0
250000,0.586282,3.8393341076267906,0.30577597,0.3356562137049942,0.3356562137049942,0.5656256,0.026001494,0.00016479187,0.1549306,0.002751037,1.0
300000,0.44457793,3.5612114577631817,0.32293904,0.2130085750775406,0.2130085750775406,0.54505074,0.026538735,0.00013406237,0.14468744,0.0022399034,1.0
350000,0.4280623,3.11463133640553,-0.02279081,-0.05414746543778802,-0.05414746543778802,0.5944301,0.02536948,0.00010333195,0.13444397,0.0017287532,1.0
400000,0.3627053,2.8190498014054386,0.06815285,0.1764436296975252,0.1764436296975252,0.56963074,0.027870268,7.260404e-05,0.12420131,0.0012176458,1.0
450000,0.29116824,3.2289605007189377,-0.0024762861,-0.02232935803095661,-0.02232935803095661,0.5663892,0.02778003,4.495e-05,0.114983305,0.0007576667,1.0
500000,0.25665367,3.4509524657290367,-0.17952098,-0.2705180701442051,-0.2705180701442051,0.51371557,0.021881487,1.729546e-05,0.10576513,0.00029767954,1.0
550000,0.21144512,3.3187078942822597,-0.3242815,-0.5227154949041285,-0.5227154949041285,None,None,None,None,None,0.0
