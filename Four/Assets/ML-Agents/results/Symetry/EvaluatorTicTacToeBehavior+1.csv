Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4141825,17.59177888022679,-0.3455951,-0.5124069148634977,-0.5124069148634977,0.33825856,0.01920816,0.00027650883,0.19216962,0.0046092635,1.0
100000,1.4066155,17.0621162874684,-0.4359138,-0.5136768688848353,-0.5136768688848353,0.34294367,0.021614254,0.00025497843,0.18499282,0.004251141,1.0
150000,1.3918979,16.129540781357093,-0.49814573,-0.5917310657859298,-0.5917310657859298,0.32406047,0.026578844,0.0002242419,0.17474727,0.003739889,1.0
200000,1.3809466,15.475123558484349,-0.57048947,-0.6814794069079335,-0.6814794069079335,0.29930353,0.02452444,0.0001934991,0.16449967,0.0032285345,1.0
250000,1.3716943,15.539861065167052,-0.5583827,-0.627361892140586,-0.627361892140586,0.295026,0.021744197,0.00016274859,0.15424952,0.0027170512,1.0
300000,1.3570486,16.06175366769021,-0.3975918,-0.45493346977761606,-0.45493346977761606,0.32305127,0.023328392,0.00013506516,0.1450217,0.0022565827,1.0
350000,1.3450279,16.408077994428968,-0.26587233,-0.3298951079057203,-0.3298951079057203,0.32563078,0.022979463,0.0001073912,0.13579702,0.0017962722,1.0
400000,1.3334265,16.790110281038775,-0.13980812,-0.17147767694606683,-0.17147767694606683,0.31831864,0.022242833,7.6640725e-05,0.12554686,0.0012847894,1.0
450000,1.3258574,16.812611328820804,-0.04074727,-0.08684850370529329,-0.08684850370529329,0.31004503,0.02371444,4.588521e-05,0.11529505,0.00077322265,1.0
500000,1.3225412,17.012972972972975,0.007835916,-0.029043693655917236,-0.029043693655917236,0.3030231,0.025222596,1.5139298e-05,0.10504641,0.00026181538,1.0
